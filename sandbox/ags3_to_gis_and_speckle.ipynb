{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from pyproj import CRS\n",
    "\n",
    "from bedrock.gi.ags.read import ags_to_dfs\n",
    "from bedrock.gi.ags.schemas import (\n",
    "    Ags3CORE,\n",
    "    Ags3GEOL,\n",
    "    Ags3HOLE,\n",
    "    Ags3ISPT,\n",
    "    Ags3WETH,\n",
    "    BaseSAMP,\n",
    ")\n",
    "from bedrock.gi.ags.transform import ags3_to_brgi\n",
    "from bedrock.gi.concatenate import concatenate_databases\n",
    "from bedrock.gi.validate import check_no_gis_brgi_database\n",
    "\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "# pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('c:/Users/joost/ReposWindows/bedrock-gi/sandbox/gi-data/kaitak/31241/GE9908.7.ags'),\n",
       " WindowsPath('c:/Users/joost/ReposWindows/bedrock-gi/sandbox/gi-data/kaitak/44751/GE-2005-03-57 rev0.ags'),\n",
       " WindowsPath('c:/Users/joost/ReposWindows/bedrock-gi/sandbox/gi-data/kaitak/47615/GE-2007-13-4 rev0.ags')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = Path.cwd()\n",
    "gi_dir = cwd / \"gi-data\" / \"kaitak\"\n",
    "gi_files = [\n",
    "    gi_dir / \"31241\" / \"GE9908.7.ags\",\n",
    "    gi_dir / \"44751\" / \"GE-2005-03-57 rev0.ags\",\n",
    "    gi_dir / \"47615\" / \"GE-2007-13-4 rev0.ags\",\n",
    "]\n",
    "gi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = CRS(2326)\n",
    "output_excel_path = cwd / \"kaitak_gi.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGS 3 data was read for Project GE/99/08.7\n",
      "This Ground Investigation data contains groups:\n",
      "['PROJ', 'HOLE', 'PTIM', 'SAMP', 'GEOL']\n",
      "\n",
      "Transforming AGS 3 groups to Bedrock tables...\n",
      "\n",
      "'Project' table was created successfully.\n",
      "\n",
      "'Location' table was created successfully.\n",
      "\n",
      "'Sample' table was created successfully.\n",
      "\n",
      "'InSitu_PTIM' table was created successfully.\n",
      "\n",
      "'InSitu_GEOL' table was created successfully.\n",
      "\n",
      "AGS 3 data was read for Project GE/2005/03.57\n",
      "This Ground Investigation data contains groups:\n",
      "['PROJ', 'HOLE', 'CDIA', 'CORE', 'DETL', 'FLSH', 'GEOL', 'HDIA', 'PTIM', 'SAMP', '?LEGD', 'UNIT', 'ABBR', 'DICT']\n",
      "\n",
      "Transforming AGS 3 groups to Bedrock tables...\n",
      "\n",
      "'Project' table was created successfully.\n",
      "\n",
      "'Location' table was created successfully.\n",
      "\n",
      "'Sample' table was created successfully.\n",
      "\n",
      "'InSitu_CDIA' table was created successfully.\n",
      "\n",
      "'InSitu_CORE' table was created successfully.\n",
      "\n",
      "'InSitu_DETL' table was created successfully.\n",
      "\n",
      "'InSitu_FLSH' table was created successfully.\n",
      "\n",
      "'InSitu_GEOL' table was created successfully.\n",
      "\n",
      "'InSitu_HDIA' table was created successfully.\n",
      "\n",
      "'InSitu_PTIM' table was created successfully.\n",
      "\n",
      "AGS 3 data was read for Project GE/2007/13.4\n",
      "This Ground Investigation data contains groups:\n",
      "['PROJ', 'HOLE', 'CDIA', 'CORE', 'DETL', 'FLSH', 'FRAC', 'GEOL', 'HDIA', 'IPRM', 'ISPT', 'PTIM', 'SAMP', 'WETH', '?LEGD', 'DICT', 'UNIT', 'ABBR']\n",
      "\n",
      "Transforming AGS 3 groups to Bedrock tables...\n",
      "\n",
      "'Project' table was created successfully.\n",
      "\n",
      "'Location' table was created successfully.\n",
      "\n",
      "'Sample' table was created successfully.\n",
      "\n",
      "'InSitu_CDIA' table was created successfully.\n",
      "\n",
      "'InSitu_CORE' table was created successfully.\n",
      "\n",
      "'InSitu_DETL' table was created successfully.\n",
      "\n",
      "'InSitu_FLSH' table was created successfully.\n",
      "\n",
      "'InSitu_FRAC' table was created successfully.\n",
      "\n",
      "'InSitu_GEOL' table was created successfully.\n",
      "\n",
      "'InSitu_HDIA' table was created successfully.\n",
      "\n",
      "'InSitu_IPRM' table was created successfully.\n",
      "\n",
      "'InSitu_ISPT' table was created successfully.\n",
      "\n",
      "'InSitu_PTIM' table was created successfully.\n",
      "\n",
      "'InSitu_WETH' table was created successfully.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brgi_db = {}\n",
    "for gi_file in gi_files:\n",
    "    with open(gi_file) as ags_file:\n",
    "        ags3_data = ags_file.read()\n",
    "    ags3_db = ags_to_dfs(ags3_data)\n",
    "    brgi_db_from_1_ags_file = ags3_to_brgi(ags3_db, crs)\n",
    "    brgi_db = concatenate_databases(brgi_db, brgi_db_from_1_ags_file)\n",
    "\n",
    "check_no_gis_brgi_database(brgi_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for table in brgi_db.values():\n",
    "    depth_to_top = table.get(\"depth_to_top\")\n",
    "    if depth_to_top is not None:\n",
    "        print(int(depth_to_top.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'HOLE'\n",
      "'SAMP'\n",
      "non-nullable series 'GEOL_GEOL' contains null values:\n",
      "76     None\n",
      "137    None\n",
      "241    None\n",
      "267    None\n",
      "Name: GEOL_GEOL, dtype: object\n",
      "Error while coercing 'ISPT_NVAL' to type int64: Could not coerce <class 'pandas.core.series.Series'> data_container into type int64:\n",
      "     index failure_case\n",
      "0       14         None\n",
      "1       26         None\n",
      "2       59         None\n",
      "3       86         None\n",
      "4       87         None\n",
      "..     ...          ...\n",
      "144    556         None\n",
      "145    557         None\n",
      "146    558         None\n",
      "147    574         None\n",
      "148    593         None\n",
      "\n",
      "[149 rows x 2 columns]\n",
      "Error while coercing 'CORE_SREC' to type int64: Could not coerce <class 'pandas.core.series.Series'> data_container into type int64:\n",
      "    index failure_case\n",
      "0       0         None\n",
      "1       1         None\n",
      "2       2         None\n",
      "3       3         None\n",
      "4       4         None\n",
      "..    ...          ...\n",
      "56    292         None\n",
      "57    293         None\n",
      "58    294         None\n",
      "59    295         None\n",
      "60    296         None\n",
      "\n",
      "[61 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Validate raw AGS 3 data with pandera schemas.\n",
    "# Use try-catch to avoid crashing the notebook, while still displaying the errors in the data.\n",
    "\n",
    "try:\n",
    "    Ags3HOLE.validate(ags3_db[\"HOLE\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    BaseSAMP.validate(ags3_db[\"SAMP\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    Ags3GEOL.validate(ags3_db[\"GEOL\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    Ags3ISPT.validate(ags3_db[\"ISPT\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    Ags3CORE.validate(ags3_db[\"CORE\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    Ags3WETH.validate(ags3_db[\"WETH\"])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel sheet names cannot contain [':', '/', '\\\\', '?', '*', '[', ']']. Replaced '?LEGD' with '_LEGD'\n",
      "Ground Investigation data has been written to 'c:\\Users\\joost\\ReposWindows\\bedrock-gi\\sandbox\\kaitak_gi.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "write_gi_dfs_to_excel(ags3_db, output_excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_column(df: pd.DataFrame, column_name: str, values) -> pd.DataFrame:\n",
    "    \"\"\"Make sure that pd.DataFrame.insert() doesn't cause errors when the column already exists.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _description_\n",
    "        column_name (str): _description_\n",
    "        values (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        df.drop(columns=column_name, inplace=True)\n",
    "    df.insert(loc=0, column=column_name, value=values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speckle Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from specklepy.api import operations\n",
    "from specklepy.api.client import SpeckleClient\n",
    "from specklepy.api.credentials import get_default_account\n",
    "from specklepy.objects import Base\n",
    "from specklepy.objects.geometry import Line, Point\n",
    "from specklepy.transports.server import ServerTransport\n",
    "\n",
    "\n",
    "def to_speckle():\n",
    "    load_dotenv()\n",
    "    stream_id = \"7fbe8ed384\"\n",
    "    hole_table_path = \"data/1_split2/HOLE.csv\"\n",
    "    df = pd.read_csv(hole_table_path, index_col=0)\n",
    "    client, stream_id = get_stream(stream_id)\n",
    "\n",
    "    # next create a server transport - this is the vehicle through which you will send and receive\n",
    "    transport = ServerTransport(client=client, stream_id=stream_id)\n",
    "\n",
    "    hash = create_hash(df, transport)\n",
    "\n",
    "    commit_hash(hash, client, stream_id)\n",
    "\n",
    "\n",
    "def get_stream(stream_id):\n",
    "    # Authenticate with Speckle server\n",
    "    speckle_server = \"app.speckle.systems\"\n",
    "    speckle_token = os.environ[\"speckle_token\"]\n",
    "    client = SpeckleClient(host=speckle_server)\n",
    "    account = get_default_account()\n",
    "\n",
    "    client.authenticate_with_token(speckle_token)\n",
    "\n",
    "    # create a new stream. this returns the stream id\n",
    "    if not stream_id:\n",
    "        stream_id = client.stream.create(name=\"a shiny new stream\")\n",
    "\n",
    "    # use that stream id to get the stream from the server\n",
    "    new_stream = client.stream.get(id=stream_id)\n",
    "    return client, stream_id\n",
    "\n",
    "\n",
    "def create_hash(df, transport):\n",
    "    newObj = Base()\n",
    "    for i, row in df.iterrows():\n",
    "        x = row[\"HOLE_NATE\"]\n",
    "        y = row[\"HOLE_NATN\"]\n",
    "        z_top = row[\"HOLE_GL\"]\n",
    "        z_bot = z_top - row[\"HOLE_FDEP\"]\n",
    "\n",
    "        # GisPointElement, GisLineElement lijken niet te werken\n",
    "        p1 = Point(x=x, y=y, z=z_top)\n",
    "        p2 = Point(x=x, y=y, z=z_bot)\n",
    "        line = Line(start=p1, end=p2)\n",
    "        relevant_cols = [\n",
    "            \"HOLE_ID\",\n",
    "            \"HOLE_TYPE\",\n",
    "            \"HOLE_STAR\",\n",
    "            \"HOLE_LOG\",\n",
    "            \"?HOLE_DLOG\",\n",
    "            \"?HOLE_CHEK\",\n",
    "            \"?HOLE_DCHK\",\n",
    "            \"HOLE_REM\",\n",
    "            \"?HOLE_FLSH\",\n",
    "            \"HOLE_ENDD\",\n",
    "            \"HOLE_BACD\",\n",
    "            \"HOLE_CREW\",\n",
    "            \"HOLE_INCL\",\n",
    "            \"HOLE_EXC\",\n",
    "        ]\n",
    "\n",
    "        for col in relevant_cols:\n",
    "            line[col] = row[col]\n",
    "\n",
    "        newObj[f\"myline{i}\"] = line\n",
    "        break\n",
    "\n",
    "    # this serialises the block and sends it to the transport\n",
    "    hash = operations.send(base=newObj, transports=[transport])\n",
    "    return hash\n",
    "\n",
    "\n",
    "def commit_hash(hash, client, stream_id):\n",
    "    # you can now create a commit on your stream with this object\n",
    "    commid_id = client.commit.create(\n",
    "        stream_id=stream_id,\n",
    "        object_id=hash,\n",
    "        message=f\"these are lines I made in speckle-py at {dt.datetime.now()}\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    to_speckle()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
